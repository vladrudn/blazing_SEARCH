use crate::document_record::DocumentIndex;
use crate::inverted_index::InvertedIndex;
use once_cell::sync::Lazy;
use regex::Regex;
use std::fs;
use std::sync::Mutex;

static WORD_REGEX: Lazy<Regex> = Lazy::new(|| Regex::new(r"\b[\p{L}\p{N}]+\b").unwrap());

static UKRAINIAN_VOWELS: &str = "–∞–µ—î–∏—ñ—ó–æ—É—é—è—å";

// –°–ª–æ–≤–Ω–∏–∫ —Å–ª—ñ–≤ –¥–ª—è –ø—Ä–∏–ø–∏–Ω–µ–Ω–Ω—è –ø–æ—à—É–∫—É –≤ —Ñ–∞–π–ª–∞—Ö "–æ—Å–æ–±–æ–≤–∏–π*"
static PERSONAL_FILE_STOP_WORDS: &[&str] = &[
    "—Å—Ç–∞—Ä—à", "–º–æ–ª–æ–¥—à", "—Å–æ–ª–¥–∞—Ç", "—Å–µ—Ä–∂–∞–Ω—Ç", "—à—Ç–∞–±", "–ª–µ–π—Ç–µ–Ω–∞–Ω—Ç", "–º–∞–π–æ—Ä", "–º–∞—Ç—Ä–æ—Å"
];

#[derive(Debug, Clone)]
pub struct SearchEngineMatch {
    pub context: String,
    pub position: usize,
}

use crate::document_record::Paragraph;

#[derive(Debug, Clone)]
pub struct SearchEngineResult {
    pub file_name: String,
    pub file_path: String,
    pub matches: Vec<SearchEngineMatch>,
    pub all_paragraphs: Vec<Paragraph>,
    pub file_size: u64,
    pub last_modified: u64,
}

#[derive(Debug)]
pub enum SearchMode {
    Quick,
    Full,
    Remaining,
}

pub struct SearchEngine {
    data: Mutex<SearchEngineData>,
}

struct SearchEngineData {
    index: DocumentIndex,
    inverted_index: Option<InvertedIndex>,
}

// –§—É–Ω–∫—Ü—ñ—è –¥–ª—è –ø–µ—Ä–µ–≤—ñ—Ä–∫–∏ —á–∏ –ü–û–ß–ò–ù–ê–Ñ–¢–¨–°–Ø –ø–∞—Ä–∞–≥—Ä–∞—Ñ –∑ –∑–∞–±–æ—Ä–æ–Ω–µ–Ω–∏—Ö —Å–ª—ñ–≤ –¥–ª—è –æ—Å–æ–±–æ–≤–∏—Ö —Ñ–∞–π–ª—ñ–≤
fn starts_with_personal_stop_words(paragraph: &str) -> bool {
    let binding = paragraph.to_lowercase();
    let lower_paragraph = binding.trim();
    PERSONAL_FILE_STOP_WORDS.iter().any(|stop_word| lower_paragraph.starts_with(stop_word))
}


impl SearchEngine {
    pub fn new() -> Self {
        Self {
            data: Mutex::new(SearchEngineData {
                index: DocumentIndex::new(),
                inverted_index: None,
            }),
        }
    }

    pub fn load_from_file(&mut self, index_path: &str) -> Result<(), String> {
        let content = fs::read_to_string(index_path)
            .map_err(|e| format!("–ü–æ–º–∏–ª–∫–∞ —á–∏—Ç–∞–Ω–Ω—è —ñ–Ω–¥–µ–∫—Å—É: {}", e))?;

        let index: DocumentIndex =
            serde_json::from_str(&content).map_err(|e| format!("–ü–æ–º–∏–ª–∫–∞ –ø–∞—Ä—Å–∏–Ω–≥—É JSON: {}", e))?;

        // –°–ø—Ä–æ–±—É—î–º–æ –∑–∞–≤–∞–Ω—Ç–∞–∂–∏—Ç–∏ —ñ–Ω–≤–µ—Ä—Ç–æ–≤–∞–Ω–∏–π —ñ–Ω–¥–µ–∫—Å
        let inverted_path = "inverted_index.json";
        let inverted_index = if std::path::Path::new(inverted_path).exists() {
            InvertedIndex::load_from_file(inverted_path).ok()
        } else {
            None
        };

        // –û–Ω–æ–≤–ª—é—î–º–æ –¥–∞–Ω—ñ –∑ –±–ª–æ–∫—É–≤–∞–Ω–Ω—è–º
        let mut data = self.data.lock()
            .map_err(|e| format!("–ü–æ–º–∏–ª–∫–∞ –±–ª–æ–∫—É–≤–∞–Ω–Ω—è –¥–∞–Ω–∏—Ö: {}", e))?;
        data.index = index;
        data.inverted_index = inverted_index;

        Ok(())
    }

    pub fn reload(&self, index_path: &str) -> Result<(), String> {
        let content = fs::read_to_string(index_path)
            .map_err(|e| format!("–ü–æ–º–∏–ª–∫–∞ —á–∏—Ç–∞–Ω–Ω—è —ñ–Ω–¥–µ–∫—Å—É: {}", e))?;

        let index: DocumentIndex =
            serde_json::from_str(&content).map_err(|e| format!("–ü–æ–º–∏–ª–∫–∞ –ø–∞—Ä—Å–∏–Ω–≥—É JSON: {}", e))?;

        // –°–ø—Ä–æ–±—É—î–º–æ –∑–∞–≤–∞–Ω—Ç–∞–∂–∏—Ç–∏ —ñ–Ω–≤–µ—Ä—Ç–æ–≤–∞–Ω–∏–π —ñ–Ω–¥–µ–∫—Å
        let inverted_path = "inverted_index.json";
        let inverted_index = if std::path::Path::new(inverted_path).exists() {
            InvertedIndex::load_from_file(inverted_path).ok()
        } else {
            None
        };

        // –û–Ω–æ–≤–ª—é—î–º–æ –¥–∞–Ω—ñ –∑ –±–ª–æ–∫—É–≤–∞–Ω–Ω—è–º
        let mut data = self.data.lock()
            .map_err(|e| format!("–ü–æ–º–∏–ª–∫–∞ –±–ª–æ–∫—É–≤–∞–Ω–Ω—è –¥–∞–Ω–∏—Ö: {}", e))?;
        data.index = index;
        data.inverted_index = inverted_index;

        Ok(())
    }

    pub async fn search(
        &self,
        query: &str,
        mode: SearchMode,
        view_mode: Option<&str>,
    ) -> Result<Vec<SearchEngineResult>, String> {
        if query.trim().is_empty() {
            return Ok(Vec::new());
        }

        // –°–ø—Ä–æ–±—É—î–º–æ –∞–≤—Ç–æ–º–∞—Ç–∏—á–Ω–æ –ø–µ—Ä–µ–∑–∞–≤–∞–Ω—Ç–∞–∂–∏—Ç–∏ —ñ–Ω–¥–µ–∫—Å–∏ —è–∫—â–æ –ø–æ—Ç—Ä—ñ–±–Ω–æ
        self.try_reload_indices_if_needed();

        let processed_query = self.process_search_query(query);
        let query_words = self.extract_search_words(&processed_query);

        if query_words.is_empty() {
            return Ok(Vec::new());
        }

        let mut results = Vec::new();

        // –û—Ç—Ä–∏–º—É—î–º–æ –¥–æ—Å—Ç—É–ø –¥–æ –¥–∞–Ω–∏—Ö
        let data = self.data.lock()
            .map_err(|e| format!("–ü–æ–º–∏–ª–∫–∞ –±–ª–æ–∫—É–≤–∞–Ω–Ω—è –¥–∞–Ω–∏—Ö: {}", e))?;

        // –í–∏–∫–æ—Ä–∏—Å—Ç–æ–≤—É—î–º–æ —ñ–Ω–≤–µ—Ä—Ç–æ–≤–∞–Ω–∏–π —ñ–Ω–¥–µ–∫—Å —è–∫—â–æ –¥–æ—Å—Ç—É–ø–Ω–∏–π
        if let Some(ref inverted_index) = data.inverted_index {
            println!("üîç –ü–æ—à—É–∫ —á–µ—Ä–µ–∑ —ñ–Ω–≤–µ—Ä—Ç–æ–≤–∞–Ω–∏–π —ñ–Ω–¥–µ–∫—Å –¥–ª—è —Å–ª—ñ–≤: {:?}", query_words);
            let (inv_docs, inv_words) = inverted_index.get_stats();
            println!("üìä –Ü–Ω–≤–µ—Ä—Ç–æ–≤–∞–Ω–∏–π —ñ–Ω–¥–µ–∫—Å: {} –¥–æ–∫—É–º–µ–Ω—Ç—ñ–≤, {} —É–Ω—ñ–∫–∞–ª—å–Ω–∏—Ö —Å–ª—ñ–≤", inv_docs, inv_words);

            // –û—Ç—Ä–∏–º—É—î–º–æ –∫–∞–Ω–¥–∏–¥–∞—Ç—ñ–≤ –¥–æ–∫—É–º–µ–Ω—Ç—ñ–≤ –∑ —ñ–Ω–≤–µ—Ä—Ç–æ–≤–∞–Ω–æ–≥–æ —ñ–Ω–¥–µ–∫—Å—É
            let candidates = inverted_index.search_fast(&query_words, &data.index, &mode);
            println!("üéØ –ó–Ω–∞–π–¥–µ–Ω–æ {} –∫–∞–Ω–¥–∏–¥–∞—Ç—ñ–≤ –¥–æ–∫—É–º–µ–Ω—Ç—ñ–≤", candidates.len());

            for (doc_idx, paragraph_positions) in candidates {
                if doc_idx < data.index.documents.len() {
                    let document = &data.index.documents[doc_idx];
                    let paragraphs = document.get_paragraphs();
                    let mut document_matches = Vec::new();

                    // –ü–µ—Ä–µ–≤—ñ—Ä—è—î–º–æ —Ç—ñ–ª—å–∫–∏ —Ç—ñ –ø–∞—Ä–∞–≥—Ä–∞—Ñ–∏, —è–∫—ñ —î –≤ –ø–æ–∑–∏—Ü—ñ—è—Ö
                    for &pos in &paragraph_positions {
                        if pos < paragraphs.len() {
                            let paragraph = &paragraphs[pos];
                            let paragraph_lower = paragraph.text.to_lowercase();

                            // –ü—Ä–æ–ø—É—Å–∫–∞—î–º–æ –ø–∞—Ä–∞–≥—Ä–∞—Ñ–∏ —è–∫—ñ –ø–æ—á–∏–Ω–∞—é—Ç—å—Å—è –∑ "–ü—ñ–¥—Å—Ç–∞–≤–∞" —Ç—ñ–ª—å–∫–∏ –≤ —Ä–µ–∂–∏–º—ñ "–í–∏—Ç—è–≥"
                            if view_mode == Some("fragments")
                                && paragraph_lower.trim().starts_with("–ø—ñ–¥—Å—Ç–∞–≤–∞")
                            {
                                continue;
                            }

                            // –ù–æ—Ä–º–∞–ª—ñ–∑—É—î–º–æ –ø–∞—Ä–∞–≥—Ä–∞—Ñ –¥–ª—è –ø–æ—à—É–∫—É (–≤–∏–¥–∞–ª—è—î–º–æ –∞–ø–æ—Å—Ç—Ä–æ—Ñ–∏)
                            let normalized_paragraph = paragraph_lower.replace('\'', "");

                            // –ü–µ—Ä–µ–≤—ñ—Ä—è—î–º–æ —á–∏ –≤—Å—ñ —Å–ª–æ–≤–∞ –¥—ñ–π—Å–Ω–æ —î –≤ —Ü—å–æ–º—É –Ω–æ—Ä–º–∞–ª—ñ–∑–æ–≤–∞–Ω–æ–º—É –ø–∞—Ä–∞–≥—Ä–∞—Ñ—ñ
                            let has_all_words = query_words
                                .iter()
                                .all(|word| normalized_paragraph.contains(word));

                            if has_all_words {
                                // –ü–µ—Ä–µ–≤—ñ—Ä—è—î–º–æ –±–ª–∏–∑—å–∫—ñ—Å—Ç—å –¥–ª—è –ü–Ü–ë
                                let is_name_search =
                                    query_words.len() >= 2 && query_words.len() <= 3;

                                let proximity_check = !is_name_search
                                    || self
                                        .check_words_proximity(&normalized_paragraph, &query_words);

                                if proximity_check {
                                    // –ó–Ω–∞–π–¥–µ–Ω–∏–π –ø–∞—Ä–∞–≥—Ä–∞—Ñ –∑ –ø–µ—Ä—Å–æ–Ω–æ—é –∑–∞–≤–∂–¥–∏ –¥–æ–¥–∞—î–º–æ (—Ñ—ñ–ª—å—Ç—Ä–∞—Ü—ñ—è –Ω–∞—Å—Ç—É–ø–Ω–∏—Ö –ø–∞—Ä–∞–≥—Ä–∞—Ñ—ñ–≤ –±—É–¥–µ –≤ JS)
                                    document_matches.push(SearchEngineMatch {
                                        context: paragraph.text.clone(),
                                        position: pos,
                                    });
                                }
                            }
                        }
                    }

                    if !document_matches.is_empty() {
                        results.push(SearchEngineResult {
                            file_name: document.file_name.clone(),
                            file_path: document.file_path.clone(),
                            matches: document_matches,
                            all_paragraphs: paragraphs,
                            file_size: document.file_size,
                            last_modified: document.last_modified,
                        });
                    }
                }
            }
        } else {
            println!("‚ö†Ô∏è  –Ü–Ω–≤–µ—Ä—Ç–æ–≤–∞–Ω–∏–π —ñ–Ω–¥–µ–∫—Å –Ω–µ –¥–æ—Å—Ç—É–ø–Ω–∏–π, –≤–∏–∫–æ—Ä–∏—Å—Ç–æ–≤—É—î–º–æ –∑–≤–∏—á–∞–π–Ω–∏–π –ø–æ—à—É–∫");
            // –ó–≤–∏—á–∞–π–Ω–∏–π –ø–æ—à—É–∫ —è–∫ —Ä–µ–∑–µ—Ä–≤–Ω–∏–π –≤–∞—Ä—ñ–∞–Ω—Ç
            for document in data.index.documents.iter() {
                let paragraphs = document.get_paragraphs();
                let mut document_matches = Vec::new();
                let mut has_any_match = false;

                for (pos, paragraph) in paragraphs.iter().enumerate() {
                    let paragraph_lower = paragraph.text.to_lowercase();

                    // –ü—Ä–æ–ø—É—Å–∫–∞—î–º–æ –ø–∞—Ä–∞–≥—Ä–∞—Ñ–∏ —è–∫—ñ –ø–æ—á–∏–Ω–∞—é—Ç—å—Å—è –∑ "–ü—ñ–¥—Å—Ç–∞–≤–∞" —Ç—ñ–ª—å–∫–∏ –≤ —Ä–µ–∂–∏–º—ñ "–í–∏—Ç—è–≥"
                    if view_mode == Some("fragments")
                        && paragraph_lower.trim().starts_with("–ø—ñ–¥—Å—Ç–∞–≤–∞")
                    {
                        continue;
                    }

                    // –ù–æ—Ä–º–∞–ª—ñ–∑—É—î–º–æ –ø–∞—Ä–∞–≥—Ä–∞—Ñ –¥–ª—è –ø–æ—à—É–∫—É (–≤–∏–¥–∞–ª—è—î–º–æ –∞–ø–æ—Å—Ç—Ä–æ—Ñ–∏)
                    let normalized_paragraph = paragraph_lower.replace('\'', "");

                    let has_all_words = query_words
                        .iter()
                        .all(|word| normalized_paragraph.contains(word));

                    if has_all_words {
                        let is_name_search = query_words.len() >= 2 && query_words.len() <= 3;

                        let proximity_check = !is_name_search
                            || self.check_words_proximity(&normalized_paragraph, &query_words);

                        if proximity_check {
                            // –ó–Ω–∞–π–¥–µ–Ω–∏–π –ø–∞—Ä–∞–≥—Ä–∞—Ñ –∑ –ø–µ—Ä—Å–æ–Ω–æ—é –∑–∞–≤–∂–¥–∏ –¥–æ–¥–∞—î–º–æ (—Ñ—ñ–ª—å—Ç—Ä–∞—Ü—ñ—è –Ω–∞—Å—Ç—É–ø–Ω–∏—Ö –ø–∞—Ä–∞–≥—Ä–∞—Ñ—ñ–≤ –±—É–¥–µ –≤ JS)
                            document_matches.push(SearchEngineMatch {
                                context: paragraph.text.clone(),
                                position: pos,
                            });
                            has_any_match = true;
                        }
                    }
                }

                if has_any_match {
                    results.push(SearchEngineResult {
                        file_name: document.file_name.clone(),
                        file_path: document.file_path.clone(),
                        matches: document_matches,
                        all_paragraphs: paragraphs,
                        file_size: document.file_size,
                        last_modified: document.last_modified,
                    });
                }
            }
        }

        // –°–æ—Ä—Ç—É—î–º–æ –∑–∞ –ø–æ—Ä—è–¥–∫–æ–º –≤ —ñ–Ω–¥–µ–∫—Å—ñ (—Ñ–∞–π–ª–∏ –≤–∂–µ –≤–ø–æ—Ä—è–¥–∫–æ–≤–∞–Ω—ñ –≤—ñ–¥ –Ω–æ–≤–∏—Ö –¥–æ —Å—Ç–∞—Ä–∏—Ö –∑–∞ –¥–∞—Ç–æ—é –∑ –Ω–∞–∑–≤–∏ —Ñ–∞–π–ª—É)
        // –ó–±–µ—Ä—ñ–≥–∞—î–º–æ —Ü–µ–π –ø–æ—Ä—è–¥–æ–∫, –ø–æ—Ç—ñ–º –∑–∞ –∫—ñ–ª—å–∫—ñ—Å—Ç—é –∑–±—ñ–≥—ñ–≤
        results.sort_by(|a, b| {
            // –ó–Ω–∞—Ö–æ–¥–∏–º–æ —ñ–Ω–¥–µ–∫—Å–∏ —Ñ–∞–π–ª—ñ–≤ –≤ –æ—Ä–∏–≥—ñ–Ω–∞–ª—å–Ω–æ–º—É —ñ–Ω–¥–µ–∫—Å—ñ
            let index_a = data
                .index
                .documents
                .iter()
                .position(|doc| doc.file_path == a.file_path);
            let index_b = data
                .index
                .documents
                .iter()
                .position(|doc| doc.file_path == b.file_path);

            match (index_a, index_b) {
                (Some(idx_a), Some(idx_b)) => {
                    // –ó–±–µ—Ä—ñ–≥–∞—î–º–æ –ø–æ—Ä—è–¥–æ–∫ (–º–µ–Ω—à–∏–π —ñ–Ω–¥–µ–∫—Å —Å–ø–æ—á–∞—Ç–∫—É = –Ω–æ–≤—ñ—à—ñ —Ñ–∞–π–ª–∏)
                    match idx_a.cmp(&idx_b) {
                        std::cmp::Ordering::Equal => {
                            // –Ø–∫—â–æ –ø–æ—Ä—è–¥–æ–∫ –æ–¥–Ω–∞–∫–æ–≤–∏–π, —Å–æ—Ä—Ç—É—î–º–æ –∑–∞ –∫—ñ–ª—å–∫—ñ—Å—Ç—é –∑–±—ñ–≥—ñ–≤
                            b.matches.len().cmp(&a.matches.len())
                        }
                        other => other,
                    }
                }
                (Some(_), None) => std::cmp::Ordering::Less, // a –∑–Ω–∞–π–¥–µ–Ω–æ, b –Ω—ñ
                (None, Some(_)) => std::cmp::Ordering::Greater, // b –∑–Ω–∞–π–¥–µ–Ω–æ, a –Ω—ñ
                (None, None) => b.matches.len().cmp(&a.matches.len()), // –æ–±–∏–¥–≤–∞ –Ω–µ –∑–Ω–∞–π–¥–µ–Ω—ñ, —Å–æ—Ä—Ç—É—î–º–æ –∑–∞ –∑–±—ñ–≥–∞–º–∏
            }
        });

        Ok(results)
    }

    fn process_search_query(&self, query: &str) -> String {
        // –í–∏–¥–∞–ª—è—î–º–æ –∞–ø–æ—Å—Ç—Ä–æ—Ñ–∏
        let without_apostrophes = query.replace('\'', "");

        // –†–æ–∑–±–∏–≤–∞—î–º–æ –Ω–∞ —Å–ª–æ–≤–∞ —Ç–∞ –æ–±—Ä–æ–±–ª—è—î–º–æ —Å—Ç–µ–º—ñ–Ω–≥
        let words: Vec<String> = without_apostrophes
            .split_whitespace()
            .map(|word| self.stem_word(word))
            .collect();

        words.join(" ")
    }

    fn extract_search_words(&self, query: &str) -> Vec<String> {
        WORD_REGEX
            .find_iter(query)
            .map(|m| m.as_str().to_lowercase())
            .collect()
    }

    fn check_words_proximity(&self, paragraph: &str, query_words: &[String]) -> bool {
        if query_words.len() < 2 {
            return true;
        }

        // –ù–æ—Ä–º–∞–ª—ñ–∑—É—î–º–æ –ø–∞—Ä–∞–≥—Ä–∞—Ñ –¥–ª—è –ø–æ—à—É–∫—É (–≤–∏–¥–∞–ª—è—î–º–æ –∞–ø–æ—Å—Ç—Ä–æ—Ñ–∏)
        let normalized_paragraph = paragraph.to_lowercase().replace('\'', "");

        // –ü–µ—Ä–µ–≤—ñ—Ä—è—î–º–æ —á–∏ –≤—Å—ñ —Å–ª–æ–≤–∞ –π–¥—É—Ç—å —É –ø—Ä–∞–≤–∏–ª—å–Ω–æ–º—É –ø–æ—Ä—è–¥–∫—É –∑ —Ä–æ–∑—É–º–Ω–æ—é –≤—ñ–¥—Å—Ç–∞–Ω–Ω—é
        let mut last_position = 0;

        for (i, word) in query_words.iter().enumerate() {
            if let Some(word_pos) = normalized_paragraph[last_position..].find(word) {
                let absolute_pos = last_position + word_pos;

                // –î–ª—è –ø–µ—Ä—à–æ–≥–æ —Å–ª–æ–≤–∞ –≤—Å—Ç–∞–Ω–æ–≤–ª—é—î–º–æ –ø–æ—á–∞—Ç–∫–æ–≤—É –ø–æ–∑–∏—Ü—ñ—é
                if i == 0 {
                    last_position = absolute_pos + word.len();
                    continue;
                }

                // –î–ª—è –Ω–∞—Å—Ç—É–ø–Ω–∏—Ö —Å–ª—ñ–≤ –ø–µ—Ä–µ–≤—ñ—Ä—è—î–º–æ –≤—ñ–¥—Å—Ç–∞–Ω—å
                let distance = absolute_pos - last_position;

                // –î–æ–∑–≤–æ–ª—è—î–º–æ –¥–æ 15 —Å–∏–º–≤–æ–ª—ñ–≤ –º—ñ–∂ —Å–ª–æ–≤–∞–º–∏ (–¥–ª—è —É—Ä–∞—Ö—É–≤–∞–Ω–Ω—è –≤—ñ–¥–º—ñ–Ω–∫—ñ–≤ —ñ —Ä–æ–∑–¥—ñ–ª–æ–≤–∏—Ö –∑–Ω–∞–∫—ñ–≤)
                // –¶–µ –¥–æ–∑–≤–æ–ª–∏—Ç—å –∑–Ω–∞–π—Ç–∏ "–î–û–ù–ê –ê–Ω–∞—Ç–æ–ª—ñ—è" –ø—Ä–∏ –ø–æ—à—É–∫—É "–¥–æ–Ω –∞–Ω–∞—Ç–æ–ª"
                if distance > 15 {
                    return false;
                }

                // –û–Ω–æ–≤–ª—é—î–º–æ –ø–æ–∑–∏—Ü—ñ—é –¥–ª—è –ø–æ—à—É–∫—É –Ω–∞—Å—Ç—É–ø–Ω–æ–≥–æ —Å–ª–æ–≤–∞
                last_position = absolute_pos + word.len();
            } else {
                // –°–ª–æ–≤–æ –Ω–µ –∑–Ω–∞–π–¥–µ–Ω–æ –ø—ñ—Å–ª—è –ø–æ–ø–µ—Ä–µ–¥–Ω—ñ—Ö - –ø–æ—Ä—è–¥–æ–∫ –ø–æ—Ä—É—à–µ–Ω–æ
                return false;
            }
        }

        true
    }

    fn stem_word(&self, word: &str) -> String {
        let word = word.to_lowercase();

        // –û–±—Ä–æ–±–∫–∞ —Å–ª—ñ–≤ –∑ –¥–µ—Ñ—ñ—Å–æ–º
        if word.contains('-') {
            let parts: Vec<String> = word
                .split('-')
                .map(|part| self.stem_word_part(part))
                .collect();
            return parts.join("-");
        }

        self.stem_word_part(&word)
    }

    fn stem_word_part(&self, word: &str) -> String {
        let mut result = word.to_string();

        // –í–∏–¥–∞–ª—è—î–º–æ –∑–∞–∫—ñ–Ω—á–µ–Ω–Ω—è -–µ—Ü—å
        if result.ends_with("–µ—Ü—å") {
            result = result[..result.len() - "–µ—Ü—å".len()].to_string();
        } else if result.ends_with("—Ü—è") {
            result = result[..result.len() - "—Ü—è".len()].to_string();
        } else if result.ends_with("—Ü—é") {
            result = result[..result.len() - "—Ü—é".len()].to_string();
        }

        // –í–∏–¥–∞–ª—è—î–º–æ –∑–∞–∫—ñ–Ω—á–µ–Ω–Ω—è -–æ–≥–æ
        if result.ends_with("–æ–≥–æ") {
            result = result[..result.len() - "–æ–≥–æ".len()].to_string();
        }
        if result.ends_with("–æ–º—É") {
            result = result[..result.len() - "–æ–º—É".len()].to_string();
        }

        // –í–∏–¥–∞–ª—è—î–º–æ –≥–æ–ª–æ—Å–Ω—ñ –≤ –∫—ñ–Ω—Ü—ñ
        while !result.is_empty() {
            if let Some(last_char) = result.chars().last() {
                if UKRAINIAN_VOWELS.contains(last_char) || last_char == '–π' {
                    result.pop();
                } else {
                    break;
                }
            } else {
                break;
            }
        }

        result
    }

    pub fn get_stats(&self) -> (usize, usize) {
        let data = self.data.lock()
            .expect("–ö—Ä–∏—Ç–∏—á–Ω–∞ –ø–æ–º–∏–ª–∫–∞ –±–ª–æ–∫—É–≤–∞–Ω–Ω—è –¥–∞–Ω–∏—Ö –ø—Ä–∏ –æ—Ç—Ä–∏–º–∞–Ω–Ω—ñ —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∏");
        (data.index.total_documents, data.index.total_words)
    }

    fn try_reload_indices_if_needed(&self) {
        let documents_path = "documents_index.json";
        let inverted_path = "inverted_index.json";

        // –ü–µ—Ä–µ–≤—ñ—Ä—è—î–º–æ —á–∏ —ñ—Å–Ω—É—é—Ç—å —Ñ–∞–π–ª–∏ —ñ–Ω–¥–µ–∫—Å—ñ–≤ —ñ —á–∏ –≤–æ–Ω–∏ –Ω–æ–≤—ñ—à—ñ –∑–∞ –ø–æ—Ç–æ—á–Ω—ñ
        if std::path::Path::new(documents_path).exists() && std::path::Path::new(inverted_path).exists() {
            let should_reload = {
                if let Ok(data) = self.data.lock() {
                    // –Ø–∫—â–æ —ñ–Ω–≤–µ—Ä—Ç–æ–≤–∞–Ω–∏–π —ñ–Ω–¥–µ–∫—Å –≤—ñ–¥—Å—É—Ç–Ω—ñ–π, –ø–µ—Ä–µ–∑–∞–≤–∞–Ω—Ç–∞–∂—É—î–º–æ
                    data.inverted_index.is_none() || data.index.documents.is_empty()
                } else {
                    return; // –ù–µ –º–æ–∂–µ–º–æ –æ—Ç—Ä–∏–º–∞—Ç–∏ –±–ª–æ–∫—É–≤–∞–Ω–Ω—è, –ø—Ä–æ–ø—É—Å–∫–∞—î–º–æ
                }
            };

            if should_reload {
                println!("üîÑ –ê–≤—Ç–æ–º–∞—Ç–∏—á–Ω–µ –ø–µ—Ä–µ–∑–∞–≤–∞–Ω—Ç–∞–∂–µ–Ω–Ω—è —ñ–Ω–¥–µ–∫—Å—ñ–≤...");
                if let Err(e) = self.reload(documents_path) {
                    println!("‚ö†Ô∏è  –ü–æ–º–∏–ª–∫–∞ –∞–≤—Ç–æ–º–∞—Ç–∏—á–Ω–æ–≥–æ –ø–µ—Ä–µ–∑–∞–≤–∞–Ω—Ç–∞–∂–µ–Ω–Ω—è —ñ–Ω–¥–µ–∫—Å—ñ–≤: {}", e);
                } else {
                    println!("‚úÖ –Ü–Ω–¥–µ–∫—Å–∏ –∞–≤—Ç–æ–º–∞—Ç–∏—á–Ω–æ –ø–µ—Ä–µ–∑–∞–≤–∞–Ω—Ç–∞–∂–µ–Ω–æ");
                }
            }
        }
    }
}
